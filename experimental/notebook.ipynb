{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On my machine (Mike) I use conda env \"trials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get protocol reference, vectorize and store in Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of separate pages: 19\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load the document - here we are just using a protocol in a specific directory\n",
    "# file_path = './documents/protocol.pdf'\n",
    "file_path = './documents/consent.pdf'\n",
    "separate_pages = []             \n",
    "loader = PyMuPDFLoader(file_path)\n",
    "page = loader.load()\n",
    "separate_pages.extend(page)\n",
    "print(f\"Number of separate pages: {len(separate_pages)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the document string: 44940\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# OyMuPDFLoader loads pages into separate docs!\n",
    "# This is a problem when we chunk because we only chunk individual\n",
    "# documents.  We need ONE overall document so that the chunks can\n",
    "# overlap between actual PDF pages.\n",
    "document_string = \"\"\n",
    "for page in separate_pages:\n",
    "    document_string += page.page_content\n",
    "print(f\"Length of the document string: {len(document_string)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 67 \n",
      "Maximum chunk size: 1031\n",
      "Length of  document: 67\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# CHOP IT UP\n",
    "def tiktoken_len(text):\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4o\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap = 50,\n",
    "    length_function = tiktoken_len\n",
    ")\n",
    "text_chunks = text_splitter.split_text(document_string)\n",
    "print(f\"Number of chunks: {len(text_chunks)} \")\n",
    "max_chunk_size = 0\n",
    "for chunk in text_chunks:\n",
    "    max_chunk_size = max(max_chunk_size, len(chunk))\n",
    "print(f\"Maximum chunk size: {max_chunk_size}\")\n",
    "document = [Document(page_content=chunk) for chunk in text_chunks]\n",
    "print(f\"Length of  document: {len(document)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x10bc89150> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x10da0c4d0> model='text-embedding-3-small' dimensions=None deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=None openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n",
      ":memory:\n",
      "http://localhost:6333\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "import defaults\n",
    "\n",
    "embedding_model = defaults.default_embedding_model\n",
    "qdrant_url = defaults.default_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection exists\n",
      "Added 0 new documents\n",
      "Skipped 67 existing documents\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "import hashlib\n",
    "\n",
    "\"\"\"\n",
    "This code creates a hash for every chunk and checks to see if that chunk already exists in the\n",
    "vector database.  We only want one collection in Qdrant, but want to make sure that if a user\n",
    "selects a document that has already been embedded and stored, it does not get stored again.  We\n",
    "also add metadata for the document title, so that we can make our retriever focus on documents of\n",
    "interest.  For example, after some usage, the application might have 20 documents for the user to \n",
    "select from.  We want the retriever to be exactly right for the documents that they selected.\n",
    "\n",
    "This could also be useful if different versions of documents are in existence.  We would not want to\n",
    "recreate a large vectorstore.  But the user could select the most recent version.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_document_hash(doc_content):\n",
    "    \"\"\"Generate a unique hash for the document content.\"\"\"\n",
    "    return hashlib.md5(doc_content.encode()).hexdigest()\n",
    "\n",
    "# Add a unique hash to your documents\n",
    "for doc in document:\n",
    "    doc.metadata['content_hash'] = get_document_hash(doc.page_content)\n",
    "\n",
    "# Add the document title\n",
    "for doc in document:\n",
    "    doc.metadata['document_title'] = file_path.split('/')[-1]\n",
    "\n",
    "client = QdrantClient(url=qdrant_url)\n",
    "\n",
    "# If the collection exists, then we need to check to see if our document is already\n",
    "# present, in which case we would not want to store it again.\n",
    "if client.collection_exists(\"protocol_collection\"):\n",
    "    print(\"Collection exists\")\n",
    "    qdrant_vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"protocol_collection\",\n",
    "        url=qdrant_url\n",
    "    )\n",
    "    \n",
    "    # Check for existing documents and only add new ones\n",
    "    existing_hashes = set()\n",
    "    new_docs = []\n",
    "    \n",
    "    # Get all existing hashes\n",
    "    scroll_filter = rest.Filter(\n",
    "        should=[\n",
    "            rest.FieldCondition(\n",
    "                key=\"metadata.content_hash\",\n",
    "                match=rest.MatchValue(value=doc.metadata['content_hash'])\n",
    "            ) for doc in document\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    scroll_results = client.scroll(\n",
    "        collection_name=\"protocol_collection\",\n",
    "        scroll_filter=scroll_filter,\n",
    "        limit=len(document)  # Adjust this if you have a large number of documents\n",
    "    )\n",
    "    \n",
    "    existing_hashes = set(point.payload.get('metadata', {}).get('content_hash') for point in scroll_results[0])\n",
    "    \n",
    "    for doc in document:\n",
    "        if doc.metadata['content_hash'] not in existing_hashes:\n",
    "            new_docs.append(doc)\n",
    "    \n",
    "    if new_docs:\n",
    "        qdrant_vectorstore.add_documents(new_docs)\n",
    "    \n",
    "    print(f\"Added {len(new_docs)} new documents\")\n",
    "    print(f\"Skipped {len(existing_hashes)} existing documents\")\n",
    "else: \n",
    "    print(\"Collection does not exist\")                           #So we go ahead and just add the documents\n",
    "    qdrant_vectorstore = QdrantVectorStore.from_documents(\n",
    "        documents=document,\n",
    "        embedding=embedding_model,\n",
    "        collection_name=\"protocol_collection\",\n",
    "        url=qdrant_url\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http import models as rest\n",
    "\n",
    "\"\"\"\n",
    "This code sets up the search type but more importantly it has the filter\n",
    "set up correctly.  We get a list of document titles that we want to include\n",
    "in the filter, and pass it into the function, returning the retriever.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def create_protocol_retriever(document_titles):\n",
    "    return qdrant_vectorstore.as_retriever(\n",
    "        search_type='mmr',                                  #mmr is experiment for me\n",
    "        search_kwargs={\n",
    "            'filter': rest.Filter(\n",
    "                must=[\n",
    "                    rest.FieldCondition(\n",
    "                        key=\"metadata.document_title\",\n",
    "                        match=rest.MatchAny(any=document_titles)\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "            'k': 5,                                         # for the mmr search it will only return k\n",
    "            'fetch_k': 50,                                  # but will evaluate fetch_k candidates\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Usage example\n",
    "document_titles = [\"consent.pdf\", \"protocol.pdf\"]\n",
    "protocol_retriever = create_protocol_retriever(document_titles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test protocol retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "--------------------------------------------------\n",
      "('49\\n'\n",
      " '9.1.2\\n'\n",
      " 'Study Procedures and Materials\\n'\n",
      " '. . . . . . . . . . . . . . . . . . . . .\\n'\n",
      " '50\\n'\n",
      " '9.1.3\\n'\n",
      " 'Potential Risks of Study Participation\\n'\n",
      " '. . . . . . . . . . . . . . . . . .\\n'\n",
      " '51\\n'\n",
      " '9.1.4\\n'\n",
      " 'Alternatives to Study Participation . . . . . . . . . . . . . . . . . . . .\\n'\n",
      " '52\\n'\n",
      " '9.2\\n'\n",
      " 'Adequacy of Protection Against Risks . . . . . . . . . . . . . . . . . . . . '\n",
      " '. .\\n'\n",
      " '52\\n'\n",
      " '9.2.1\\n'\n",
      " 'Parental Permission, Informed Consent and Assent . . . . . . . . . . .\\n'\n",
      " '52\\n'\n",
      " '9.2.2\\n'\n",
      " 'Institutional Review Board and Human Research Protection\\n'\n",
      " '. . . . . .\\n'\n",
      " '54\\n'\n",
      " '9.2.3')\n",
      "--------------------------------------------------\n",
      "Document 2\n",
      "--------------------------------------------------\n",
      "('study, you will need to have an IV tube (catheter) available through which '\n",
      " 'to give the study drug.\\n'\n",
      " 'If for some reason you do not have an IV catheter available during the time '\n",
      " 'period in which you are\\n'\n",
      " 'getting the study drug, a new one would need to be placed. Getting new IV '\n",
      " 'catheters is common in\\n'\n",
      " 'patients with sepsis. Getting IV catheters may cause pain, lightheadedness '\n",
      " 'and fainting, bleeding,\\n'\n",
      " 'bruising, or swelling at the puncture site. Infection is a rare '\n",
      " 'possibility.\\n'\n",
      " 'Risk of developing anemia: Many patients who are critically ill require a '\n",
      " 'blood transfusion as\\n'\n",
      " 'part of their treatment. Any time we draw blood from a patient it can '\n",
      " 'contribute to a low red blood\\n'\n",
      " 'cell count (or anemia). The volume of blood taken for the purposes of this '\n",
      " 'study, however, is very\\n'\n",
      " 'small compared to the amount taken in the course of routine ICU care. We '\n",
      " 'will stop drawing blood')\n",
      "--------------------------------------------------\n",
      "Document 3\n",
      "--------------------------------------------------\n",
      "('does not mean that you give up any of your legal rights to seek compensation '\n",
      " 'for your injuries.\\n'\n",
      " 'COST AND COMPENSATION\\n'\n",
      " 'While you are in this study, the cost of your usual medical care - '\n",
      " 'procedures, medications and\\n'\n",
      " 'doctor visits - will continue to be billed to you or your insurance. There '\n",
      " 'will be no additional costs\\n'\n",
      " 'to you for your participation in this study. You will not receive any '\n",
      " 'payments for taking part in\\n'\n",
      " 'this study. Any tests that are performed as a result of being in this study '\n",
      " 'will be paid for by the\\n'\n",
      " 'research doctor. The National Institutes of Health is providing funding for '\n",
      " 'this study.\\n'\n",
      " 'For your time and inconvenience, you (study participant) will receive $25 '\n",
      " 'per survey completion\\n'\n",
      " 'up to a total of $50. You will be issued a debit card specially designed for '\n",
      " 'clinical research. When\\n'\n",
      " 'a survey has been completed, funds will be approved and automatically loaded '\n",
      " 'onto your card.\\n'\n",
      " 'University of Utah\\n'\n",
      " 'Institutional Review Board')\n",
      "--------------------------------------------------\n",
      "Document 4\n",
      "--------------------------------------------------\n",
      "('definitely related to the study drug.\\n'\n",
      " '• The occurrence, in any subject, of a life-threatening SAE whose causal '\n",
      " 'relationship to\\n'\n",
      " 'study drug is judged to be probable or definite.\\n'\n",
      " '• Two (2) occurrences of Grade 3 or higher toxicities that are judged to be '\n",
      " 'probably or\\n'\n",
      " 'definitely related to the study drug.\\n'\n",
      " '• Two (2) occurrences of a clinically significant Grade 3 or higher '\n",
      " 'laboratory abnormality\\n'\n",
      " 'that are judged to be probably or definitely related to study drug.\\n'\n",
      " 'After notification of the NICHD Program Official or Project Officer, and the '\n",
      " 'DSMB chairper-\\n'\n",
      " 'son, of serious, unexpected, and study-related adverse events or '\n",
      " 'unanticipated problems (UP),\\n'\n",
      " 'decisions will be made whether to continue the study without change, and '\n",
      " 'whether to convene\\n'\n",
      " 'the entire DSMB for an emergent meeting. If a decision is made to suspend '\n",
      " 'enrollment in the')\n",
      "--------------------------------------------------\n",
      "Document 5\n",
      "--------------------------------------------------\n",
      "('it can be harmful. If we see that your white blood cell count is greater '\n",
      " 'than 50,000 cells/mm3\\n'\n",
      " 'we will not give the study drug. We expect this to happen in less than 10% '\n",
      " 'of patients.\\n'\n",
      " '• Allergic reactions: Allergic reactions can happen with any drug. If you '\n",
      " 'are known to be\\n'\n",
      " 'allergic to GM-CSF then you cannot be in the study. We expect this to happen '\n",
      " 'very rarely\\n'\n",
      " 'if at all (in less than 1% of patients).\\n'\n",
      " 'Risks associated with anakinra treatment (if you receive anakinra): Anakinra '\n",
      " 'has been\\n'\n",
      " 'safely used in many studies of critically ill adults and children in doses '\n",
      " 'that are the same or higher\\n'\n",
      " 'than those used in this study. There are, however, a few side effects that '\n",
      " 'we will be looking for.\\n'\n",
      " 'There is no way to say without a doubt that you will or will not experience '\n",
      " 'any of these or other\\n'\n",
      " 'side effects:')\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "risks = protocol_retriever.get_relevant_documents(\"Risks of study\")\n",
    "len(risks)\n",
    "for i, doc in enumerate(risks, 1):\n",
    "    print(f\"Document {i}\")\n",
    "    print(\"-\" * 50)\n",
    "    pprint(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_tokens=0\n",
    "# for risk in risks:\n",
    "#     total_tokens+= tiktoken_len(risk.page_content)\n",
    "# print(f\"Tokens in context: {total_tokens}\")\n",
    "# for i, doc in enumerate(risks, 1):\n",
    "#     print(f\"Document {i}\")\n",
    "#     print(\"-\" * 50)\n",
    "#     pprint(doc.page_content)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benefits = protocol_retriever.get_relevant_documents(\"Benefits of study\")\n",
    "# for i, doc in enumerate(benefits, 1):\n",
    "#     print(f\"Document {i}\")\n",
    "#     print(\"-\" * 50)\n",
    "#     pprint(doc.page_content)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
